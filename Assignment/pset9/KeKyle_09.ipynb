{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "with open('w09-data.out') as f:\n",
    "    lines = f.readlines()\n",
    "    arcs = np.array(list(map(lambda x : x.strip().split(), lines[1:11])))\n",
    "    reads = np.array(list(map(lambda x : x.strip().split(), lines[13:23])))\n",
    "\n",
    "arc_composition = arcs[:,3]\n",
    "arc_lengths = [len(arc) for arc in arc_composition]\n",
    "arc_names = arcs[:,0]\n",
    "read_names = reads[:,0]\n",
    "supplement_counts = reads[:,1].astype(int)\n",
    "supplement_counts = pd.Series(supplement_counts)\n",
    "supplement_counts.index = read_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write a simulator as a positive control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_simulator(arc_loc_sturcture,tau):\n",
    "    \n",
    "    # intialize list of generated reads\n",
    "    all_reads = []\n",
    "    length = [len(arc) for arc in arc_loc_sturcture]\n",
    "    # convert tau to nu\n",
    "    nu = (np.array(length) * np.array(tau)) / (np.sum(np.array(length) * np.array(tau)))\n",
    "    # Choose a transcript isoform i with probability ν_i, according to the nucleotide abundances; i.e. P(T)=ν.\n",
    "    isoform = np.random.choice(arc_loc_sturcture,size = 1000000, p=nu)\n",
    "    for i in range(0,1000000):\n",
    "        \n",
    "        #Given isoform i, we choose one of the segments j=0..Li−1 of Ti uniformly -- i.e. \n",
    "        # for isoform Ti of length Li, we sample its segments with probability 1/Li.\n",
    "        all_reads.append(np.random.choice(list(isoform[i].lower())))\n",
    "\n",
    "    return all_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 generated reads:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'i',\n",
       " 'b',\n",
       " 'c',\n",
       " 'b',\n",
       " 'd',\n",
       " 'd',\n",
       " 'd',\n",
       " 'd',\n",
       " 'h',\n",
       " 'c',\n",
       " 'c',\n",
       " 'e',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'f',\n",
       " 'e',\n",
       " 'd']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "test_tau = np.random.dirichlet(np.ones(10),size=1).ravel()\n",
    "all_reads = read_simulator(arc_composition,test_tau)\n",
    "print('First 20 generated reads:')\n",
    "all_reads[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I first sampled probability vector $\\tau$ of size 10 as my test case for $\\tau$, where the probability vector $\\tau$ sum up to one. This set of $\\tau$ is then passed into my simulator function to generate a positive control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the log likelihhod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "def log_likelihood(r_count,arc_loc_sturcture,tau=None,nu=None):\n",
    "    # Get length of each transcript\n",
    "    length = np.array([len(arc) for arc in arc_loc_sturcture])\n",
    "    # convert transcript abundance to nucletoide abundance\n",
    "    if type(tau) == np.ndarray:\n",
    "        nu = (np.array(length) * np.array(tau)) / (np.sum(np.array(length) * np.array(tau)))\n",
    "    prob_s_given_g_all = 1 / length \n",
    "    \n",
    "    # this is a variable tracking the cumlative summation of log likelihoods\n",
    "    total_log_cum = 0\n",
    "\n",
    "    # Iterate over each read\n",
    "    for read in r_count.index:\n",
    "        # This variable stores the log joint-probabilities of P(S,T ∣ ν,L) for the 10 genes given a particiular read (a...j)\n",
    "        log_prob_joint = []\n",
    "        # Iterate through the 10 genes, if a read is in a gene then P(S,T ∣ ν,L) is calculated and stored in log_prob_joint\n",
    "        for gene,abund in enumerate(nu):\n",
    "            if read in arc_loc_sturcture[gene].lower():\n",
    "                prob_s_given_g = prob_s_given_g_all[gene]\n",
    "                log_prob_joint.append((np.log(prob_s_given_g) + np.log(abund)))\n",
    "\n",
    "        # Sum up log(P(S∣ν,L)) multiplied by number of times this read appears in the read counts.\n",
    "        total_log_cum +=  logsumexp(log_prob_joint) * r_count[read]\n",
    "           \n",
    "             \n",
    "    return total_log_cum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above algorithm of calculating the log-likelihood given read counts and $\\tau$ is described as the following steps:\n",
    "### First the passed in $\\tau$ is converted to $\\nu$. Then we calculate P(S∣T,L) through 1/L. Next we initalize a variable called total_log_cum that stores the cumlative summation of log likelihoods. We then enter into the main algorithm ---- a nested for loop structure. In the outer loop, we iterate over each read (a..j), we then initialize a intermediate variable called log_prob_joint to store the log joint-probabilities P(S,T ∣ ν,L) for the 10 genes given a particiular read (a...j). We then loop through each gene, and if the read exists in the gene, we calculate P(S,T ∣ ν,L) to store into log_prob_joint. After each inner for loop is terminated, we marginalize P(S,T | v, L) to get log(P(S∣ν,L)) and multiply it to the number of counts for that particular read and add the result to the cumlative summation of log likelihoods. The resulting cumlative summation of log likelihood after the nested for loop is termintaed is the total log likelihood we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood of my generated test case\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2217763.8797335"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = log_likelihood(pd.Series(all_reads).value_counts().sort_index(),arc_composition,tau=test_tau)\n",
    "print('Log-likelihood of my generated test case')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood from Lestrade's approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181268.66666667 116255.33333333 120803.          89508.25\n",
      "  78638.25        49317.58333333  34077.25        75900.\n",
      " 125137.33333333 129094.33333333]\n",
      "Arc1        0.133\n",
      "Arc2        0.170\n",
      "Arc3        0.118\n",
      "Arc4        0.065\n",
      "Arc5        0.057\n",
      "Arc6        0.048\n",
      "Arc7        0.050\n",
      "Arc8        0.111\n",
      "Arc9        0.122\n",
      "Arc10       0.126\n"
     ]
    }
   ],
   "source": [
    "with open('w09-data.out') as f:\n",
    "    #   The first line is \"The <n> transcripts of the sand mouse Arc locus\"\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'^The (\\d+) transcripts', line)\n",
    "    T     = int(match.group(1))\n",
    "\n",
    "    # The next T lines are \n",
    "    #   <Arcn>  <true_tau> <L> <structure>\n",
    "    # tau's may be present, or obscured (\"xxxxx\")\n",
    "    tau       = np.zeros(T)\n",
    "    L         = np.zeros(T).astype(int)\n",
    "    tau_known = True   # until we see otherwise\n",
    "    for i in range(T):\n",
    "        fields    = f.readline().split()\n",
    "        if fields[1] == \"xxxxx\":\n",
    "            tau_known = False\n",
    "        else:\n",
    "            tau[i] = float(fields[1])\n",
    "        L[i]      = int(fields[2])\n",
    "\n",
    "    # after a blank line,\n",
    "    # 'The <n> read sequences':\n",
    "    line  = f.readline()\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'The (\\d+) read sequences', line)\n",
    "    N     = int(match.group(1))\n",
    "\n",
    "    # the next T lines are \n",
    "    #  <read a-j> <count>\n",
    "\n",
    "\n",
    "r = pd.Series(all_reads).value_counts().sort_index().ravel()\n",
    "\n",
    "\n",
    "S = T    # S = R = T : there are T transcripts (Arc1..Arc10), S segments (A..J), R reads (a..j)\n",
    "R = T\n",
    "Slabel   = list(string.ascii_uppercase)[:S]               # ['A'..'J']        : the upper case labels for Arc locus segments \n",
    "Tlabel   = [ \"Arc{}\".format(d) for d in range(1,T+1) ]    # ['Arc1'..'Arc10'] : the labels for Arc transcript isoforms\n",
    "Rlabel   = list(string.ascii_lowercase)[:T]               # ['a'..'j']        : lower case labels for reads\n",
    "\n",
    "\n",
    "# Count how often each segment A..J is used in the isoforms i\n",
    "# We'll use that to split observed read counts across the isoforms\n",
    "# that they might have come from.\n",
    "#\n",
    "segusage = np.zeros(S).astype(int)\n",
    "for i in range(T):\n",
    "    for j in range(i,i+L[i]): \n",
    "        segusage[j%S] += 1\n",
    "\n",
    "\n",
    "# Naive analysis:\n",
    "#\n",
    "c  = np.zeros(T)\n",
    "for i in range(T):\n",
    "    for k in range(i,i+L[i]):\n",
    "        c[i] += (1.0 / float(segusage[k%S])) * float(r[k%S])  # For each read k, assume read k-> segment j,\n",
    "                                                              # and assign 1/usage count to each transcript\n",
    "                                                              # that contains segment j.\n",
    "Z       = np.sum(c)\n",
    "est_nu  = np.divide(c, Z)       # nucleotide abundance\n",
    "\n",
    "print(c)\n",
    "\n",
    "est_tau = np.divide(est_nu, L)  # convert to TPM, transcript abundance\n",
    "est_tau = np.divide(est_tau, np.sum(est_tau))\n",
    "\n",
    "# Print a table of the resulting estimates for tau\n",
    "for i in range(T):\n",
    "    print (\"{0:10s}  {1:5.3f}\".format(Tlabel[i], est_tau[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lestrade's Tau:\n",
      "-2240807.3180777426\n",
      "Difference in log-likelihood: truth vs Lestrade's Tau\n",
      "23043.438344242517\n"
     ]
    }
   ],
   "source": [
    "lestrade_log_likelihood = log_likelihood(pd.Series(all_reads).value_counts().sort_index(),arc_composition,est_tau)\n",
    "print(\"Lestrade's Tau:\")\n",
    "print(lestrade_log_likelihood)\n",
    "print(\"Difference in log-likelihood: truth vs Lestrade's Tau\")\n",
    "print(test - lestrade_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that the log likelihood from the test-case true $\\tau$ is greater than the log likelihood from Lestrade's estimate $\\tau$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. estimate isoform abundances by EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My estimated Tau to the read counts in Lestrade et al. supplementary data file:\n",
      "Arc1     0.148088\n",
      "Arc2     0.497867\n",
      "Arc3     0.001895\n",
      "Arc4     0.048769\n",
      "Arc5     0.037587\n",
      "Arc6     0.019331\n",
      "Arc7     0.006772\n",
      "Arc8     0.134462\n",
      "Arc9     0.054924\n",
      "Arc10    0.050305\n",
      "dtype: object\n",
      "Sum of Arc1 and Arc2 abundances:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.645954862736237"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def expectation(r_count,arc_loc_sturcture,prob_s_given_g_all,nu):\n",
    "\n",
    "\n",
    "    posterior_numerator =  pd.DataFrame(columns = arc_names,index = r_count.index)\n",
    "    \n",
    "    for read in r_count.index:\n",
    "        # Iterate through the 10 genes, if a read is in a gene then P(S,T ∣ ν,L) is calculated and stored in log_prob_joint\n",
    "        for gene,abund in enumerate(nu):\n",
    "            if read in arc_loc_sturcture[gene].lower():\n",
    "                prob_s_given_g = prob_s_given_g_all[gene]\n",
    "                posterior_numerator.loc[read,arc_names[gene]] = (prob_s_given_g * abund)\n",
    "    \n",
    "    # obtaining the joint posterior probabilities by dividing each joint probability by the \n",
    "    # marginalized probability over each gene\n",
    "    \n",
    "    joint_posterior = posterior_numerator.div(posterior_numerator.sum(axis=1), axis=0)\n",
    "    # expected count given posterior probaiblities P(v,|S,L)\n",
    "    expected_count = joint_posterior.multiply(r_count,axis=0).sum(axis=0)\n",
    "\n",
    "    return expected_count\n",
    "\n",
    "\n",
    "def Maximization(expected_count,r_count):\n",
    "    updated_nu = expected_count / r_count.sum()\n",
    "    return updated_nu\n",
    "\n",
    "def em_algorithm(r_count,arc_loc_sturcture,initial_nu):\n",
    "    \n",
    "    # intialize random nus\n",
    "\n",
    "    prev_log_likelihood = log_likelihood(r_count,arc_loc_sturcture,nu=initial_nu)\n",
    "\n",
    "    length = np.array([len(arc) for arc in arc_loc_sturcture])\n",
    "    # convert transcript abundance to nucletoide abundance\n",
    "    prob_s_given_g_all = 1 / length \n",
    "    \n",
    "    new_log_likelihood = 10000000\n",
    "    iter = 0\n",
    "    while np.abs(new_log_likelihood - prev_log_likelihood) > 0.01:\n",
    "        prev_log_likelihood = new_log_likelihood\n",
    "        \n",
    "        if iter == 0:\n",
    "            expected_count = expectation(r_count,arc_loc_sturcture,prob_s_given_g_all,nu = initial_nu)\n",
    "        else:\n",
    "            expected_count = expectation(r_count,arc_loc_sturcture,prob_s_given_g_all,nu = updated_nu)\n",
    "\n",
    "        updated_nu = Maximization(expected_count,r_count)\n",
    "     \n",
    "        new_log_likelihood = log_likelihood(r_count,arc_loc_sturcture,nu=updated_nu)\n",
    "        iter+=1\n",
    "\n",
    "    tau = (updated_nu / length) / (np.sum((updated_nu / length)))\n",
    "    return tau, new_log_likelihood\n",
    "\n",
    "np.random.seed(42)\n",
    "# Running EM with 20 different sets of random nus\n",
    "all_estimated_tau = []\n",
    "all_log_likelihood = []\n",
    "for i in range(0,20):\n",
    "  \n",
    "    random_nu = np.random.dirichlet(np.ones(10),size=1).ravel()\n",
    "    my_estimated_tau, my_log_likelihood = em_algorithm(supplement_counts,arc_composition,initial_nu = random_nu)\n",
    "    all_estimated_tau.append(my_estimated_tau)\n",
    "    all_log_likelihood.append(my_log_likelihood)\n",
    "\n",
    "best_run = np.argmax(all_log_likelihood)\n",
    "my_estimated_tau = all_estimated_tau[best_run]\n",
    "print('My estimated Tau to the read counts in Lestrade et al. supplementary data file:')\n",
    "print(my_estimated_tau)\n",
    "print('Sum of Arc1 and Arc2 abundances:')\n",
    "my_estimated_tau[0] + my_estimated_tau[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I ran the EM algorithm with 20 different seed and obtained the $\\tau$ estimates that yielded the highest log-likelihood. We observe the most abundant two transcripts is Arc 2 and Arc 1. They account for around 65% of the population. The two least abundant transcripts are Arc 3 and Arc 7. I do not agree with Lestrade et al.'s conclusion that both Arc 2 and Arc 3 are strongly on. Arc 2 has the most abundance and Arc 3 has the least abundance. Thus, this seems to support the original belief that Arc 2 and Arc 3 are oppositely regulated. Lestrade's method is wrong because Lestrade assumed read counts are assigned to possible isoforms uniformly. This is not an assumption for the EM algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf03e3dae6cd90215c49561c10ab9ba5e35cdc7a8fe6139efc815ef068c0abf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
